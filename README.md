## Part 2: Data Provenance Detection for Image Autoregressive Models

**CISPA European Championship 2026** | Stockholm | February 14-15, 2026

Identify which generative model (VAR/RAR) produced an image, or classify as outlier.

---

## The Challenge

**Task**: Given an image, determine if it was generated by one of 8 models or is an outlier:
- **VAR models**: VAR-16, VAR-20, VAR-24, VAR-30 (depths)
- **RAR models**: RAR-B, RAR-L, RAR-XL, RAR-XXL (sizes)
- **Outlier**: Unknown source

---

## Our Approach

Based on **"Data Provenance for Image Auto-Regressive Generation"** (Zhao et al., ICLR 2026)

### Core Insight
Image Autoregressive Models (IARs) leave "fingerprints":
- Generated images = sampled from **discrete codebook entries**
- Natural images = **continuous**, not constrained by codebook
- Distance to codebook = **provenance signal**

### Implementation (Following Paper)

**1. Extract Provenance Features** (16 total)

For each model, compute 2 features:

```python
# VAR: QuantLoss + EncLoss (Equations 5, 10)
def compute_var_features(vae, images):
    f = vae.encoder(images)           # Encode
    f_q = vae.quantize(f)              # Quantize to codebook
    
    quant_loss = ||f - f_q||¬≤          # How far from codebook?
    
    recon1 = vae.decoder(f_q)          # Reconstruct once
    recon2 = vae.decoder(vae.encoder(recon1))  # Twice (calibration)
    enc_loss = ||recon1 - x||¬≤ / ||recon2 - recon1||¬≤
    
    return [quant_loss, enc_loss]

# RAR: NLL + Prob (marginalized over labels)
def compute_rar_features(tokenizer, generator, images):
    tokens = tokenizer.encode(images)
    logp = generator(tokens, labels)   # Try top-k ImageNet labels
    nll = -logp_marginalized
    return [nll, exp(-nll)]
```

**2. Train Simple Classifier**
```python
clf = LogisticRegression()  # Paper did not specify what simple classifier, so we tried to implement logistic regression
clf.fit(features, labels)   # 16-D features ‚Üí 9 classes
```

**3. Detect Outliers**
```python
probs = clf.predict_proba(features)
outliers = (probs.max(axis=1) < threshold)  # Low confidence = outlier
```

### Why It Works

| Component | Why | Paper Evidence |
|-----------|-----|----------------|
| **QuantLoss** | Generated images closer to codebook | Equation 5, Figure 1 |
| **Decoder Inversion** | Original encoder trained on natural images | Section 3.3.1, Table 4 |
| **Calibration** | Normalizes for image complexity | Equation 10, Table 5 |
| **Simple Classifier** | Features already well-separated | Section 4.2, Table 1 |

**Paper Results**: 99.2-100% TPR@1%FPR across all models

---

## Project Structure
```
‚îú‚îÄ‚îÄ solution.py          # Main implementation (paper methodology)
‚îú‚îÄ‚îÄ diagnostic.py        # Setup verification
‚îú‚îÄ‚îÄ requirements.txt     # Dependencies
‚îú‚îÄ‚îÄ VAR/                 # VAR models + checkpoints
‚îú‚îÄ‚îÄ RAR/                 # RAR models + checkpoints
‚îú‚îÄ‚îÄ dataset/             # train/val/test images
‚îî‚îÄ‚îÄ .cache/              # Feature cache (auto-created)
```

---

## What We Learned

### Technical
1. **Feature engineering > model complexity** - Simple features from model internals beat end-to-end
2. **Decoder inversion is critical** - Original encoder doesn't invert well for generated images (Table 4)
3. **Caching saves time** - Feature extraction: 30-60min ‚Üí cache ‚Üí 2min subsequent runs
4. **Simple classifiers sufficient** - Well-separated features don't need Random Forests
5. **Error handling matters** - Silent failures waste hours; fail loudly early

### From Paper to Practice
```python
# Paper says (Section 3.3.1):
"The original encoder E is not a close inversion of D for generated images"

# Our implementation:
def load_var_vae(depth):
    vae = build_vae_var(...)
    # Finetune encoder on generated images (50k, 10-50 epochs)
    # Result: TPR improved from 6.2% ‚Üí 100% (Table 4)
```

### Hackathon Lessons
- **Read paper carefully** before coding - saved hours of wrong approaches
- **Start simple, iterate** - Dummy features ‚Üí VAR only ‚Üí VAR+RAR ‚Üí calibration
- **Use diagnostic tools** - `diagnostic.py` caught setup issues early
- **Cache everything** - Don't recompute expensive features

---

## üìö References

**Primary**: Zhao, B., et al. (2026). *Data Provenance for Image Auto-Regressive Generation*. ICLR 2026.

**Models**: VAR (Tian+ 2024), RAR (Yu+ 2024), LlamaGen (Sun+ 2024), Taming (Esser+ 2021)

**Related**: LatentTracer (Wang+ 2024), AEDR (Wang+ 2025), MIA (Kowalczuk+ 2025)

---

## Acknowledgments

- **CISPA** Helmholtz Center for Information Security
- Paper authors for excellent methodology
- Team members for 24-hour sprint
---

## Part 3: Chimera Generation Challenge

###  The Challenge in 30 Seconds

**Task**: Create images that get **different predictions** on different hardware backends  
**Why it's hard**: Chimera "pockets" are 1-10,000 ULP wide in a 3,072-dimensional space  
**My result**: Found chimeras with ~0.5% success rate using boundary optimization  

---

### What Are Chimeras?

**Chimeras** are images that produce **conflicting predictions** when the same model runs on different linear algebra backends:
- üçé **Apple Accelerate** ‚Üí predicts "Cat"
- üî∑ **Intel MKL** ‚Üí predicts "Dog"  
- üü¢ **Nvidia CUDA** ‚Üí predicts "Bird"
- üî∂ **BLIS** ‚Üí predicts "Cat"

This happens because floating-point arithmetic differs slightly across hardware implementations. At decision boundaries, tiny numerical differences (10^-6 to 10^-8) cause different class predictions.

![Chimera Concept](task3/images/chimera.png)
*Figure: Same image ‚Üí Different predictions on different backends*

---

### Challenge Requirements

- **Submit**: 1,000 images (200 unique + replicates)
- **Score**: Percentage of images that are chimeras
- **Goal**: Maximize chimera count
- **Difficulty**: Chimera pockets are extremely rare (like finding needles in a haystack)

---

### My Solution: 3-Stage Pipeline

#### üéØ Stage 1: Find Decision Boundaries

Optimize images to sit exactly between two classes:
```python
def get_to_perfect_boundary(x_base, max_iters=1500):
    """Push image to decision boundary where P(class1) ‚âà P(class2)"""
    
    # Minimize gap between top-2 class probabilities
    top2_probs, _ = torch.topk(probs, 2, dim=1)
    gap = torch.abs(top2_probs[0, 0] - top2_probs[0, 1])
    
    # Loss: 80% boundary + 20% cross-entropy
    loss = 0.8 * gap + 0.2 * ce_loss
```

**Result**: Images with gap < 0.01 (almost perfectly uncertain)

---

#### üî¨ Stage 2: Dense Neighborhood Sampling

Generate 50 variations around each boundary with **9 different noise scales**:
```python
# Cover 1-10,000 ULP range systematically
noise_scales = [0.0005, 0.001, 0.002, 0.003, 
                0.005, 0.008, 0.01, 0.015, 0.02]

for scale in noise_scales:
    noise = torch.randn_like(x_boundary) * scale
    x_variant = quantize(x_boundary + noise)
```

**Why**: Chimera pockets exist at different scales; need variety to find them

---

#### üìä Stage 3: Uncertainty-Based Replication

Identify most likely chimeras and replicate them:
```python
# Find most uncertain images
uncertainties.sort(key=lambda x: x['gap'])  # Smallest gap first

# Replicate top-10 most uncertain √ó 100 copies = 1,000 images
```

**Hypothesis**: Smallest confidence gap ‚Üí Highest chimera probability

---

### üìà Results

| Approach | Chimeras Found | Success Rate |
|----------|----------------|--------------|
| Random baseline | 0 | 0.0% |
| Basic boundary search | 1 | 0.5% |
| **Dense multi-scale** | **Target: 10-20** | **5-10%** |

---

### ‚úÖ What Worked

1. **Aggressive boundary optimization** (1500 iterations, gap < 0.01)
2. **Multiple noise scales** (9 scales from 0.0005 to 0.02)
3. **Always quantize** to 8-bit (`torch.round(x * 255) / 255.0`)
4. **Replicate uncertain images** (smallest gap = most likely chimera)

### ‚ùå What Didn't Work

1. **Random perturbations** ‚Üí 0% success rate
2. **Single noise scale** ‚Üí Missed pockets at other scales
3. **Too few iterations** ‚Üí Didn't reach tight boundaries
4. **Ignoring quantization** ‚Üí Wrong boundaries

---

### üí° Key Insights

1. **Finding the boundary ‚â† finding chimeras**  
   Optimization gets you close, but you need dense sampling to hit the pocket

2. **Chimera pockets are multi-scale**  
   Can be 1 ULP or 10,000 ULP wide ‚Üí need variety in noise

3. **Quantization changes everything**  
   Floating-point boundaries ‚â† 8-bit quantized boundaries

4. **This is a real problem**  
   Not theoretical ‚Äî chimeras exist and affect production models

---

### üóÇÔ∏è Code Structure
```
task3/
‚îú‚îÄ‚îÄ generate_submissions.py      # Main: boundary optimization + dense sampling
‚îú‚îÄ‚îÄ find_and_replicate.py        # Identify & replicate uncertain images  
‚îú‚îÄ‚îÄ deep_analysis.py             # Multi-criteria uncertainty analysis
‚îî‚îÄ‚îÄ visualize_boundary_v2.py     # Decision boundary visualization
```

---

### üöÄ Future Improvements

1. **Adaptive noise scaling**: Refine around promising regions
2. **Ensemble boundaries**: Optimize multiple class pairs simultaneously
3. **Gradient-free search**: Basin-hopping or genetic algorithms
4. **Exploit locality**: Once you find one chimera, search nearby
5. **Hardware-in-the-loop**: Test on real backends during generation

---

**TL;DR**: Found chimeras by (1) optimizing to decision boundaries, (2) densely sampling with multi-scale noise, (3) replicating most uncertain candidates. Achieved ~0.5% chimera rate, proving numerical instability is a measurable problem in deep learning.


